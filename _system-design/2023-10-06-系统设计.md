---
title: 系统设计
author: Irol Zhou
date: 2023-10-06
category: system-design
layout: post
---

## 设计原则

![1727426239507](../img/系统设计.png)

## 高可用

### 集群多活

#### 负载均衡

- 轮询

- 随机

- IP-Hash

- 加权轮询

  - 计算所有权重的和`totalWeight`
  - 初始化每个节点的当前权重`currentWeight = 0`
  - 每次选择前，更新节点的当前权重`currentWeight += weight`，这里的weight是节点自身的权重
  - 选择`currentWeight `最大的节点，选择后更新该节点`currentWeight -= totalWeight`
  ![img](../img/加权.png)
- 插件式



### 服务保护

#### 资源隔离

**线程隔离**

Hystrix实现线程隔离，通过自定义线程池，将接口请求通过线程池进行隔离

```java
@HystrixCommand(fallbackMethod = "fallback",
            groupKey = "consumer-group",
            commandKey = "hello",
            commandProperties = {
                    @HystrixProperty(name = "execution.isolation.strategy", value = "THREAD")
            },
            threadPoolKey = "threadPool-hello",
            threadPoolProperties = {
                    @HystrixProperty(name = "coreSize", value = "2"),
                    @HystrixProperty(name = "maximumSize", value = "2"),
                    @HystrixProperty(name = "maxQueueSize", value = "1"),
                    @HystrixProperty(name = "keepAliveTimeMinutes", value = "0"),
                    @HystrixProperty(name = "queueSizeRejectionThreshold", value = "10")
            }
    )
    @GetMapping("")
    public Object hello(@RequestParam("name") String name) {
        return "hello, " + name;
    }
```

**进程隔离**

即拆分系统为子系统进行物理隔离

#### 限流

##### 单机限流-RateLimiter

1）自定义限流注解

```java
@Target(ElementType.METHOD)
@Retention(RetentionPolicy.RUNTIME)
@Documented
public @interface Limit {

    // 资源的key,唯一
    // 作用：不同的接口，不同的流量控制
    String key() default "";

    // 最多的访问限制次数
    double permitsPerSecond();

    // 获取令牌最大等待时间
    long timeOut();

    // 获取令牌最大等待时间,单位(例:分钟/秒/毫秒) 默认:毫秒
    TimeUnit timeUnit() default TimeUnit.MILLISECONDS;
}
```

2）增加限流切面

```java
@Slf4j
@Aspect
@Component
public class LimitAspect {

    private static final String LIMIT_ERROR_MESSAGE = "系统繁忙，请稍后重试";

    private final Map<String, RateLimiter> limitMap = Maps.newConcurrentMap();

    @Around("@annotation(cn.net.susan.annotation.Limit)")
    public Object around(ProceedingJoinPoint joinPoint) throws Throwable {
        MethodSignature signature = (MethodSignature) joinPoint.getSignature();
        Method method = signature.getMethod();
        //拿limit的注解
        Limit limit = method.getAnnotation(Limit.class);
        if (limit != null) {
            String key = limit.key();
            RateLimiter rateLimiter = null;
            //验证缓存是否有命中key
            if (!limitMap.containsKey(key)) {
                // 创建令牌桶
                rateLimiter = RateLimiter.create(limit.permitsPerSecond());
                limitMap.put(key, rateLimiter);
                log.info("新建了令牌桶={}，容量={}", key, limit.permitsPerSecond());
            }
            rateLimiter = limitMap.get(key);
            // 拿令牌
            boolean acquire = rateLimiter.tryAcquire(limit.timeOut(), limit.timeUnit());
            // 拿不到命令，直接返回异常提示
            if (!acquire) {
                throw new BusinessException(LIMIT_ERROR_MESSAGE);
            }
        }
        return joinPoint.proceed();
    }
}
```

3）使用自定义注解

```java
@Limit(key = "test", permitsPerSecond = 1, timeOut = 500)
@GetMapping("/test")
public Object test(@RequestParam(value = "name") String name) {
    return query(name);
}
```

4）当有多个限流Key时，可以定义一个静态Map，将key-RateLimiter对象存储在Map中进行管理

##### 单机限流-Sentinel

###### 微服务场景

- 直接在sentinel控制台定义限流策略，但开源的sentinel控制台不会直接同步到nacos中，重启后配置清空
- 故一般在nacos中定义限流规则，并配置sentinel的数据源为nacos，这样修改nacos中的配置就可以实时更新

```yaml
spring:
  cloud:
    sentinel:
      transport:
        dashboard: localhost:8090
        port: 8719
        clientIp: 192.168.130.1
      http-method-specify: true # 开启请求方式前缀
      datasource: # 将sentinel配置持久化到nacos配置中心，具体配置字段需要查看源码
        ds1:
          nacos:
            username: nacos
            password: nacos
            server-addr: localhost:8848 # nacos地址
            dataId: cart-degrade.json  # nacos配置的dataId
            groupId: hmall  # nacos配置的groupId
            data-type: json # 配置文件类型
            rule-type: degrade # 规则类型
feign:
  sentinel:
    enabled: true # 开启feign对sentinel的支持
```

1）定义簇点链路（可配置规则的单元）

```java
@SentinelResource(value="queryMyCartsHandler", blockHandler = "queryMyCartsBlockHandler", blockHandlerClass = CartBlockHandler.class)
public List<CartVO> queryMyCarts() {
    ...
}
```

2）sentinel配置中开启请求方式前缀后（如上配置），可以将controller中restful风格的接口也展示在粗点链路中

3）feign开启了sentinel支持后，也可以在簇点链路中查看到相关信息

![img](../img/限流1.png)

4）在nacos中定义限流策略，添加相关配置，具体配置的字段需要查看sentinel源码再去查找对应解释，sentinel官网对此并没有相关介绍

###### 单体服务场景

由于sentinel控制台本身并没有持久化机制，要使用sentinel控制台还得对其进行改造，所以我们可以在代码中定义sentinel相关规则

```java
private static void initFlowQpsRule() {
    List<FlowRule> rules = new ArrayList<FlowRule>();
    FlowRule rule1 = new FlowRule();
    rule1.setResource(KEY);
    // set limit qps to 20
    rule1.setCount(20);
    rule1.setGrade(RuleConstant.FLOW_GRADE_QPS);
    rule1.setLimitApp("default");
    rules.add(rule1);
    FlowRuleManager.loadRules(rules);
}
```

###### sentinel相关配置说明

sentinel官网：https://sentinelguard.io/zh-cn/docs/introduction.html

// todo

##### 分布式限流-Redis

通过redis实现分布式限流，主要还是通过lua脚本去redis获取一个token，不同的生成token方式代表了不同的限流算法

**实现步骤：**

1）定义Enable注解

```java
@Target(ElementType.TYPE)
@Retention(RetentionPolicy.RUNTIME)
@Documented
@Import({LimitConfig.class})
public @interface EnableLimit {
}
```

2）定义限流类型枚举类

```java
@Getter
@AllArgsConstructor
public enum LimitTypeEnum {

    // 针对访问接口的所有请求
    ALL(0, "所有"),
    
    // 针对访问接口的指定IP
    IP(1, "用户ip"),
    
    // 针对访问接口的指定用户
    USER_ID(2, "用户ID");
    
    // 枚举值
    private Integer value;
    
    // 枚举描述
    private String desc;
}
```

3）定义limit注解

```java
@Target(ElementType.METHOD)
@Retention(RetentionPolicy.RUNTIME)
@Documented
public @interface Limit {

    // 资源的key,唯一
    // 作用：不同的接口，不同的流量控制
    String key() default "";

    // 最多的访问限制次数
    int permitsPerSecond();

    // 获取令牌最大等待时间
    long timeOut();

    // 获取令牌最大等待时间,单位(例:分钟/秒/毫秒) 默认:毫秒
    TimeUnit timeUnit() default TimeUnit.MILLISECONDS;

    // 限流类型，默认是整个接口
    LimitTypeEnum limitType() default LimitTypeEnum.ALL;
}
```

4）定义限流配置，该类没有用@Configuration注解，需要在启动类上使用上面定义的@EnableLimit注解启用

```java
public class LimitConfig {

    @Bean
    public LimitAspect limitAspect() {
        return new LimitAspect();
    }

    @Bean
    public DefaultRedisScript<Long> limitScript() {
        DefaultRedisScript<Long> redisScript = new DefaultRedisScript<>();
        redisScript.setScriptText(limitScriptText());
        redisScript.setResultType(Long.class);
        return redisScript;
    }

    /**
     * 限流lua脚本
     */
    private String limitScriptText() {
        return "local key = KEYS[1]\n" +
                "local count = tonumber(ARGV[1])\n" +
                "local time = tonumber(ARGV[2])\n" +
                "local current = redis.call('get', key);\n" +
                "if current == nil then\n current = 0\n end\n" +
                "if current and tonumber(current) > count then\n" +
                "    return tonumber(current);\n" +
                "end\n" +
                "current = redis.call('incr', key)\n" +
                "if tonumber(current) == 1 then\n" +
                "    redis.call('expire', key, time)\n" +
                "end\n" +
                "return tonumber(current);";
    }
}
```

5）定义限流切面

```java
@Slf4j
@Aspect
public class LimitAspect {

    private static final String LIMIT_ERROR_MESSAGE = "系统繁忙,请稍后再试";

    @Autowired
    private StringRedisTemplate stringRedisTemplate;
    @Autowired
    private DefaultRedisScript redisScript;

    @Around("@annotation(cn.net.susan.annotation.Limit)")
    public Object around(ProceedingJoinPoint joinPoint) throws Throwable {
        MethodSignature signature = (MethodSignature) joinPoint.getSignature();
        Method method = signature.getMethod();
        //拿limit的注解
        Limit limit = method.getAnnotation(Limit.class);
        if (limit != null) {
            String limitKey = buildLimitKey(limit, method);
            int count = limit.permitsPerSecond();
            long time = limit.timeOut();
            Object number = stringRedisTemplate.execute(redisScript, Lists.newArrayList(limitKey), count, time);
            if (Objects.nonNull(number) && number instanceof Long) {
                if (((Long) number).longValue() > count) {
                    throw new BusinessException(LIMIT_ERROR_MESSAGE);
                }
            }
        }
        return joinPoint.proceed();
    }

    private String buildLimitKey(Limit limit, Method method) {
        HttpServletRequest httpServletRequest = ((ServletRequestAttributes) RequestContextHolder.getRequestAttributes())
                .getRequest();
        switch (limit.limitType()) {
            case IP:
                String ipAddr = IpUtil.getIpAddr(httpServletRequest);
                return String.format("limitRate:ip:%s_%s", method.getName(), ipAddr);
            case USER_ID:
                JwtUserEntity currentUserInfo = FillUserUtil.getCurrentUserInfo();
                if (Objects.nonNull(currentUserInfo)) {
                    throw new BusinessException("请先登录");
                }
                return String.format("limitRate:userId:%s_%s", method.getName(), currentUserInfo.getId());
            default:
                return String.format("limitRate:all:%s", method.getName());
        }
    }
}
```

6）启用限流注解

```java
@EnableLimit
@SpringBootApplication(scanBasePackages = {"cn.test"})
public class ApiApplication {

    public static void main(String[] args) {
        SpringApplication.run(ApiApplication.class, args);
    }
}
```

```java
@Limit(key = "test", permitsPerSecond = 1, timeOut = 500, limitType = LimitTypeEnum.IP)
@GetMapping("/test")
public Object test(@RequestParam(value = "name") String name) {
    return query(name);
}
```

###### 固定窗口算法

上面的lua脚本实现即是固定窗口算法，**不同的算法实现由lua脚本完成**

```lua
local key = KEYS[1]
local count = tonumber(ARGV[1])
local time = tonumber(ARGV[2])
local current = redis.call('get', key)
if current == nil then	-- redis中没有该key，初始化计数为0
    current = 0
end
if current and tonumber(current) > count then	-- key的计数已经超过设定值，返回现有的计数
    return tonumber(current)
end
current = redis.call('incr', key)	-- 计数未超过设定值，计数+1
if tonumber(current) == 1 then		-- 第一次设定key值时，设置key的超时时间，这个时间即为固定窗口
    redis.call('expire', key, time) -- 当key未过期表示在这个窗口内计数，key过期后从头开启新的窗口
end
return tonumber(current)
```

###### 滑动窗口算法

滑动窗口通过zset进行实现，核心逻辑是

- 通过zset记录每个请求的时间戳，根据时间戳则可以形成一条时间线
- 新请求过来时，根据当前时间往前倒退一个窗口，移除窗口前的请求，剩下的就是最新一个窗口的请求数，由此判断是否允许请求通过

```lua
local key = KEYS[1]
local windowSize = tonumber(ARGV[1])  -- 滑动窗口的大小，单位是秒
local maxCount = tonumber(ARGV[2])  -- 窗口内最大允许的请求数
local currentTime = tonumber(ARGV[3])  -- 当前时间的时间戳（毫秒）

-- 移除窗口外的请求记录
redis.call('zremrangebyscore', key, 0, currentTime - windowSize * 1000)

-- 获取当前窗口内的请求数
local count = redis.call('zcard', key)

if count < maxCount then
    -- 如果请求数未超限，添加当前请求记录
    redis.call('zadd', key, currentTime, currentTime)
    return count + 1
else
    -- 如果请求数超限，返回 -1 表示限流
    return -1
end

```

###### 令牌桶算法

参考RateLimiter的实现方式，根据时间差补充令牌数量，核心思想是：

- 计算当前时间与上次获取令牌的时间差，根据时间差 * 令牌生成速度计算需要补充多少令牌
- 计算桶中剩余令牌数+补充令牌数，总数不能超过桶容量，根据计算出的令牌数判断是否放行请求

```lua
local key = KEYS[1]
local capacity = tonumber(ARGV[1])   -- 桶的最大容量
local refill_rate = tonumber(ARGV[2]) -- 令牌生成速率（每秒生成的令牌数）
local current_time = tonumber(ARGV[3]) -- 当前时间（单位：秒）
local tokens = tonumber(redis.call('GET', key) or 0)
local last_refill = tonumber(redis.call('GET', key .. ':last_refill') or 0)

-- 计算补充的令牌数量
if current_time > last_refill then
    local elapsed_time = current_time - last_refill
    local new_tokens = math.floor(elapsed_time * refill_rate)
    tokens = math.min(capacity, tokens + new_tokens)
    redis.call('SET', key, tokens)
    redis.call('SET', key .. ':last_refill', current_time)
end

-- 检查是否有令牌可用
if tokens > 0 then
    redis.call('DECR', key)
    return 1  -- 允许通过
else
    return 0  -- 拒绝请求
end
```

###### 漏桶算法

漏桶算法的实现思路是

- 将请求放入漏桶，然后以固定速度减去漏桶中的计数
- 根据（当前时间 - 上次漏水的时间） * 漏水速度计算应该减去的计数，如果剩余计数+请求计数<桶容量则放行

```lua
local key = KEYS[1]            -- Redis 键名
local capacity = tonumber(ARGV[1]) -- 漏桶容量
local leakRate = tonumber(ARGV[2]) -- 漏桶漏水速率（每秒处理的请求数）
local now = tonumber(ARGV[3])  -- 当前时间戳（毫秒）
local requested = tonumber(ARGV[4]) -- 请求的数量（通常为1）

-- 获取当前漏桶中的水量和最后一次漏水时间
local bucket = redis.call('HMGET', key, 'water', 'timestamp')
local water = tonumber(bucket[1]) or 0
local lastLeak = tonumber(bucket[2]) or now

-- 计算时间差
local delta = (now - lastLeak) / 1000

-- 计算漏掉的水量
local leaked = math.floor(delta * leakRate)

-- 更新漏桶中的水量
water = math.max(0, water - leaked)
lastLeak = lastLeak + (leaked / leakRate) * 1000

-- 检查是否能处理当前请求
if water + requested > capacity then
    -- 请求被拒绝
    return 0
else
    -- 更新漏桶状态
    water = water + requested
    redis.call('HMSET', key, 'water', water, 'timestamp', lastLeak)
    -- 设置过期时间
    redis.call('EXPIRE', key, math.ceil(capacity / leakRate))
    -- 请求被接受
    return 1
end
```

###### Redis内置限流功能

// todo



##### 分布式限流-Nginx

###### 限制并发数

- 使用`$binary_remote_addr`则是对每个客户端IP进行限制

```nginx
http {
    # 定义一个限流区域，这里使用客户端的 IP 地址作为限流的键值
    # $binary_remote_addr 是客户端 IP 地址的二进制表示，用于节省空间
    # zone=connlimit:10m 定义了名为 connlimit 的限流区域，10m 表示为这个区域分配 10MB 内存
    # 这个内存区域将用于跟踪并发连接状态
    limit_conn_zone $binary_remote_addr zone=connlimit:10m;

    server {
        # ...
        location / {
            # 应用限流规则到指定的 location
            # zone=connlimit 指定使用之前定义的限流区域 connlimit
			# connections=3 表示每个客户端 IP 允许的最大并发连接数为 3
            limit_conn connlimit 3;

            # 其他 location 配置 ...
        }

        # 其他 server 配置 ...
    }
}
```

- 可以定义一个全局key，对全局连接数进行限制

```nginx
http {
    limit_conn_zone $global_key zone=global_zone:10m;  # 定义一个全局的 limit_conn 区域
    server {
        listen 80;
        location / {
            limit_conn global_zone 1000;  # 限制全局并发连接数为 1000
        }
    }
}
```

###### 限制请求数

- 对客户端IP进行限制
  - `burst`：一个缓存区，允许在超过处理速度时的请求将放入这个缓冲区，超过burst的请求返回503
  - `nodelay`：立即处理burst中的请求，如果没有配置nodelay，则burst中的请求则会被延迟处理

```nginx
http {
    # 定义限制区域
    limit_req_zone $binary_remote_addr zone=mylimit:10m rate=1r/s;

    server {
        listen 80;
        server_name example.com;

        location / {
            # 应用请求速率限制
            limit_req zone=mylimit burst=5 nodelay;

            # 正常的代理或其他配置
            proxy_pass http://my_backend;
        }
    }
}
```

- 对总请求数进行限制

```nginx
http {
    limit_req_zone $server_name zone=mylimit:10m rate=1r/s;

    server {
        listen 80;
        server_name example.com;
        location / {
            limit_req zone=mylimit burst=5 nodelay;
            proxy_pass http://my_backend;
        }
    }
}
```

##### 分布式限流-Sentinel

sentinel集群限流提供两种方式，嵌入模式与独立模式

###### 嵌入模式

- 选择一个应用实例作为token server，其他实例作为token client
- 无需单独部署server，但需要限制token server所在实例的qps，避免token server实例过载
- token server与client可以通过api接口或`ClusterStateManager.applyState(ClusterStateManager.CLUSTER_SERVER/CLIENT)`进行切换
- 适合给应用集群内部限流（k8s部署情况下，每个实例代码都一样，指定server与灵活切换server都很困难）

![image](../img/嵌入模式.png)

###### 独立模式

- 独立部署一个token server（实际生产需要高可用）
- 适合给应用集群提供限流服务

![image](../img/独立模式.png)

**使用流程（独立模式）**

官方github里面提供了demo示例代码，根据demo代码进行验证

- 配置中心

官方推荐将集群流控的配置通过动态规则源进行管理，即将配置放在配置中心进行维护，token server与client接入配置中心进行规则获取与更新，这里使用nacos作为配置中心和持久化手段

- sentinel控制台

sentinel控制台上有“集群流控”的页面，但是官方开源版的sentinel控制台**并不直接支持在页面上进行集群流控配置**，如果要在控制台页面上进行操作，需要对sentinel控制台进行改造，将sentinel控制台接入配置中心，在页面修改配置后，配置会推送到配置中心，再由配置中心推送给应用。

改造说明：[Sentinel 控制台（集群流控管理）](https://github.com/alibaba/Sentinel/wiki/Sentinel-%E6%8E%A7%E5%88%B6%E5%8F%B0%EF%BC%88%E9%9B%86%E7%BE%A4%E6%B5%81%E6%8E%A7%E7%AE%A1%E7%90%86%EF%BC%89)demo里给的样例是接入nacos，将ruleProvider和rulePublisher改成nacos的实现类，这两个类的代码实现在sentinel-dashboard官方项目源码的test目录下有。

这里不做相关改造验证

- 相关概念

namespace：Token Server里的概念，代表一个应用/服务，默认为project.name

flowId：集群限流规则的id，全局唯一（自己指定）

- 配置Token Server

1. 引入sentinel-cluster相关依赖

2. 复制官方demo（sentinel-demo-cluster-server-alone）相关代码，核心在`DemoClusterServerInitFunc` （使用SPI机制，resource/META-INF/services下的文件也要复制）

3. 官方token server是通过main函数启动，这里将其写入ApplicationRunner，启动后自动执行

4. token server需要接入nacos

5. 在nacos中添加相关配置，这里使用的Group为`SENTINEL_GROUP`，namespace指定为`irol-sentinel`

   1. `cluster-server-namespace-set`：namespace列表，所有的namespace都维护在里面

      ```json
      [
          "irol-sentinel"
      ]
      ```

   2. `cluster-server-transport-config`：token server通信配置，指定token server用与client通信时使用的端口与空闲时间

      ```json
      {
          "port":11111,
          "idleSeconds":600
      }
      ```

6. 启动token server，与sentinel控制台通信端口指定为9000

- 配置Token Client

1. 引入sentinel-cluster相关依赖

2. 复制官方demo（sentinel-demo-cluster-embedded）相关代码，核心在`DemoClusterInitFunc` （使用SPI机制，resource/META-INF/services下的文件也要复制）

3. 修改DemoClusterInitFunc中的init方法

   ```java
   @Override
   public void init() throws Exception {
       // Register client dynamic rule data source.
       // 注册限流规则
       initDynamicRuleProperty();
       // Register token client related data source.
       // Token client common config:
       // client通信配置
       initClientConfigProperty();
       // Token client assign config (e.g. target token server) retrieved from assign map
       // 配置token server连接地址
       initClientServerAssignProperty();
   	// 手动指定服务为token client
       ClusterStateManager.applyState(ClusterStateManager.CLUSTER_CLIENT);
   
   }
   ```

4. token client需要接入nacos

5. 在nacos中添加相关配置，前面指定了namespace为irol-sentinel，相关配置为namespace-xxx

   1. `irol-sentinel-cluster-client-config`：client通信配置

      ```json
      {
          "requestTimeout":2000
      }
      ```

   2. `irol-sentinel-flow-rules`：限流规则，这里配置限流qps为1

      ```json
      [
          {
              "resource": "consumerHello", //需要限流的资源名称
              "grade": 1, //限流阈值类型，QPS 或线程数模式，默认为QPS
              "count": 1, //限流阈值
              "strategy": 0, //调用关系限流策略：0直接、1关联、2链路,默认直接
              //流控效果（0直接拒绝 / 1Warm Up / 2匀速排队），不支持按调用关系限流，默认直接拒绝
              "controlBehavior": 0, 
              "clusterMode": true, //标识是否为集群限流配置
              "clusterConfig": {
                  "flowId": 9959231232121334, //全局唯一的规则 ID，由集群限流管控端分配
                  "thresholdType": 1, //阈值模式，默认（0）为单机均摊，1 为全局阈值
                  //在 client 连接失败或通信失败时，是否退化到本地的限流模式
                  "fallbackToLocalWhenFail": true, 
                  "strategy": 0, //调用关系限流策略：直接、链路、关联,默认直接
                  "windowIntervalMs": 1000 //滑动窗口时间，默认1s
              }
          }
      ]
      ```

   3. `irol-sentinel-cluster-map`：token server连接配置

      ```json
      [
          {
              "machineId": "192.168.130.1@9000", // token server ip@sentinel控制台通信端口
              "ip": "192.168.130.1",	// token server ip
              "port": 11111,	// token server用来与client通信的端口
              "clientSet": [	// token client ip@sentilen控制台通信端口
                  "192.168.130.1@8720",
                  "192.168.130.1@9004",
                  "192.168.130.1@9005"
              ]
          }
      ]
      ```

   4. demo中的`initClientServerAssignProperty()`在配置token server连接地址时可能失败，JSON.parseObject解析配置失败（debug才知道），有两种解决办法

      1. 手动解析配置为JSONObject并设置到ClusterGroupEntity对象中

      2. 使用其他配置方式，需要在nacos添加配置`irol-sentinel-token-server-config`

         ```json
         {
             "serverHost":"192.168.130.1",
             "serverPort":11111
         }
         ```

         然后修改`initClientServerAssignProperty()`方法

         ```java
         ReadableDataSource<String, ClusterClientAssignConfig> clientAssignDs = new NacosDataSource<>(remoteAddress, groupId, "irol-sentinel-token-server-config"
                 , source -> JSON.parseObject(source, new TypeReference<ClusterClientAssignConfig>() {
         }));
         ```

   5. 记得在需要限流的地方加上@SentinelResource，并添加降级方法

   6. 启动token client，按上面的配置这里启动三个实例，分别使用8720，9004，9005与sentinel控制台通信

- 验证

登录sentinel控制台可以看到token server 与 client的实例信息，在token server的集群流控这里能看到token server的信息，这里显示的连接数为0，不管他，不影响使用，重点看client的信息

![1716966432627](../img/1716966432627.png)

在token client的集群流控页面能看到连接的token server信息，点击管理能看到client列表，在流控规则这里能看到在nacos配置的流控规则

**注意这里不要在页面上修改规则**，因为我们的控制台没做集群流控的改造，这里修改了规则后，推送到应用的规则会变成单机限流规则，之前的集群限流规则就失效了

![1716966590414](../img/1716966590414.png)

![1716966628282](../img/1716966628282.png)

![1716966673270](../img/1716966673270.png)

测试一下，用jmeter压一下，qps稳定在1

![1716966825938](../img/1716966825938.png)

在nacos修改限流qps为5，qps变成5，显示上有时候不是5应该是跟统计的时间段有关系

![1716967338086](../img/1716967338086.png)

**引入的问题**

token server如何实现高可用？？

思路：使用zookeeper管理token server集群，启动时一起创建节点，谁创建的节点编号最小谁是token server，其他实例监听token server的服务状态，当token server不可用时重新选举（争抢节点或取第二小的编号）；

定时对实例进行健康探测，剔除不可用的实例，实例恢复后检测自身状态然后自动重新加入；

在token server不可用时集群会退化成单机限流，理论上选举的过程中也有托底的限流策略，但选举完成后新的token server没有记录当前限流状态，且由单机限流恢复到集群限流，是否表示新token server可用时应用会处于无限流的状态，从而当有流量进来时重新发放token？如果是这样的话，那实例之间如何做数据同步？

###### 网关限流

在微服务模式下，通过sentinel在网关处对服务进行限流，也能实现分布式限流的效果



#### 熔断





#### 降级



#### 超时与重试





## 高并发

#### 服务拆分

##### 分库分表

##### 读写分离

#### 缓存

##### Redis与Mysql数据一致性

##### ORM缓存

#### 池化技术

#### 异步解耦

#### 消息队列









## 秒杀系统设计